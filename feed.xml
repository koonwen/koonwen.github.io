<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.5">Jekyll</generator><link href="https://koonwen.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://koonwen.github.io/" rel="alternate" type="text/html" /><updated>2024-10-04T06:27:15+00:00</updated><id>https://koonwen.github.io/feed.xml</id><title type="html">(core dumped)</title><subtitle>Portfolio and Blog
</subtitle><author><name>Koon Wen Lee</name></author><entry><title type="html">Efficient Disk concurrency through eio</title><link href="https://koonwen.github.io/2024/07/29/efficient-concurrency-through-eio.html" rel="alternate" type="text/html" title="Efficient Disk concurrency through eio" /><published>2024-07-29T01:02:00+00:00</published><updated>2024-07-29T01:02:00+00:00</updated><id>https://koonwen.github.io/2024/07/29/efficient-concurrency-through-eio</id><content type="html" xml:base="https://koonwen.github.io/2024/07/29/efficient-concurrency-through-eio.html"><![CDATA[<p>Getting concurrency right is often tricky. Recently, I’ve been playing
around with <strong>eio</strong>, an IO concurrency library for OCaml. In particular,
I was inspired by a <a href="https://discuss.ocaml.org/t/how-to-write-an-efficient-cp-r-clone-with-eio/14848">discussion post</a> asking how to implement an efficient directory copy using eio.
The post attracted many suggestions and I was curious to understand which
optimizations benefits eio the most. Turns out, this problem was
a one-way ticket down the rabbit hole… Here’s how it went!</p>

<h2 id="concurrency">Concurrency</h2>

<p>To get our definitions straight, we’ll define concurrency
as the structuring of a program into separate independent tasks. Each task
runs one at a time but can be interleaved with one another. However, this
overlapping structure doesn’t provide any speedup on it’s own. It needs to
be paired with a workload that consists of some waiting on some slow
device IO and other independent work that could be processed in the
meantime. To give a concrete example, a concurrent way to write to two
files to disk would be:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>t1: Write to file A
t2: While waiting for write to A to complete ...  | Write to file B
t3: While waiting for write to B to complete ...  | Writing to file A completes
t4: Writing to file B completes
</code></pre></div></div>

<h3 id="overview-of-backends-concurrency-models">Overview of backends concurrency models</h3>

<p>In order to enable concurrency, modern operating systems provide two
asynchronous IO models to pick from: readiness-based IO and
completion-based IO.</p>

<p><strong>Readiness-based IO</strong>, is provided through the use of non-blocking file
descriptors. Such descriptors respond in one of two ways when IO is requested
on them. If data is ready to be consumed, then the request is processed as per
usual. If data is not yet available, the request returns immediately with an
error code to indicate this so that the process returns to check again later
for data.</p>

<p><strong>Completion-based IO</strong> in contrast, is accomplished by using specific
asynchronous API’s which hand-off IO requests entirely to the OS and
returns immediately. Internally, this typically involves requests being
queued and monitored by an external kernel thread to process them in
a separate execution context. Upon completion the data, the OS is
responsible for notifying the requesting process.</p>

<p>Recognizing when to use one or the other is subtle but worth thinking about.
Primarily, readiness-based IO only guarantees that the program will not block
on IO but have unpredictable latency. If not properly accounted for, programs
could end up handling an arbritrary long requests on ready file descriptors,
thus causing poor latency. On the other hand, completion-based IO is more
predictable in this regard since requests are handled outside of the
process execution. However this model generally has lower throughput since
requests now have to be threaded through the OS.</p>

<h3 id="eio">eio</h3>

<p>The eio library exposes high-level structured concurrency constructs for
programming with asynchronous IO. Eio’s main abstraction are Fiber’s which
are programmatic way to express independent threads of work. Eio currently
supports running on 3 systems, windows, posix and linux. In order to push
performance as far as we can, we’ll focus on linux which is based on
io-uring, a sophisticated and fast backend based on completion-based IO.</p>

<h2 id="gotchas">Gotchas!</h2>

<p>Before we can dive into any optimizations, here are some immediate gotchas
to keep in mind.</p>

<h3 id="write-not-as-slow-as-youd-think">write: not as slow as you’d think</h3>

<p>Those with experience programming with non-blocking IO may have realized
that we’re stepping into an exceptional case. Our copy directory problem
works with regular files and the man pages for <code class="language-plaintext highlighter-rouge">open (2)</code> state:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>O_NONBLOCK or O_NDELAY
“Note that this flag has no effect for regular files and block devices;
that is, I/O operations will (briefly) block when device activity is
required, regardless of whether O_NONBLOCK is set.”
</code></pre></div></div>

<p>Well, this seems like an obvious win for us at the completion based IO
camp. However, this leaves out several important details. In particular,
O_NONBLOCK doesn’t work for regular files because writes go directly to
the page cache. Making many individual small requests for IO (in our case - to disk)
is extremely expensive. To guard against this, kernel developers implement
an efficient mechanism to transparently batch write’s into a bigger
request that is later flushed to disk. The man pages for <code class="language-plaintext highlighter-rouge">write (2)</code>
confirms this behaviour</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    A successful return from write() does not make any guarantee that
    data has been committed to disk.  On some filesystems, including
    NFS, it does not even guarantee that space has successfully been
    reserved for the data.  In this case, some errors might be
    delayed until a future write(), fsync(2), or even close(2).  The only
    only way to be sure is to call fsync(2) after you are done
    writing all your data.
</code></pre></div></div>

<p>As a consequence, a call to write does not suspend because it is readily able
to write data into the page cache. Since our copy problem is not concerned with
latency, issuing sequential writes end up being quite efficient because the
program doesn’t need to wait. The only time a write may block is when there
are no free pages left and dirty pages are flushed to disk to reclaim space.
Though in practice, this behaviour is usually not an issue.</p>

<p>All of this context was provided to now circle back to how io-uring
handles this. As described, using completion-based IO to perform writes
can be less performant than the sequential version because of the extra
infrastructure required to handle queued requests. io-uring is designed to
make smarter choices about IO and does not purely queue request. If uring
notices that an IO operation can be completed immediately (e.g. write to
a page cache), it will do so inline instead of generating an async request
for it. Conversely, if a write needs to be suspended, it then adds it to
the async queue. This hybrid architecture gives us the best of both worlds
for regular file IO. If so desired, a uring request can be set to always
generate an async request if low latency is required.</p>

<p>This discussion is not complete without mentioning the implications on
persistence. Having peered into the implementation, we now realize that
the OS may lie to us about having written something to disk. For certain
applications, this can be detail is important to have some gurantees about
their programs semantics if a failure occurs. The way to ensure that
a write has made it to disk is the call <code class="language-plaintext highlighter-rouge">fsync</code> which forces dirty pages
to be written to disk. This may or may not be a required property of real
programs so our benchmarks will consider both cases.</p>

<h3 id="read-faster-than-expected">read: faster than expected</h3>

<p>Following the discovery that regular files are blocking, surely reading
files will be slow since there is no way around having to fetch data from
disk. This is true, but the kernel has other tricks up it’s sleeve. When
requesting data for files, the kernel performs a “readahead” optimization. As
the name indicates, the kernel prefetches more data into main memory than the
program had asked for. This way, future reads are more likely to hit cache
rather of requiring a disk transfer at every read. Even better, some
applications are designed to work with data read once into memory and are
kept around to be reused directly from RAM. From a benchmarking
perspective, we should also include this in our test using a hot and cold
page cache.</p>

<h2 id="baseline-expectations">Baseline Expectations</h2>

<p>It’s useful to first get some idea of the theoretical throughput of the
system.</p>

<h3 id="hardware-capabilities">Hardware Capabilities</h3>

<p>The disk on my machine is the Intel Pro 6000p NVMe PCIe M.2 512GB.
I couldn’t find information about the transfer rates on the spec sheet so
I used public benchmarks of random mixed read write transfers which came
out as <strong>40.9 MB/s</strong>. <a href="https://ssd.userbenchmark.com/SpeedTest/289714/INTEL-SSDPEKKF512G7H">ref</a></p>

<h3 id="measurements-with-fio">Measurements with fio</h3>

<p>Now to check where we are at on my machine, I used the <code class="language-plaintext highlighter-rouge">fio</code> benchmarking
tool to measure the throughput of random read writes.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  fio --name=expect_fsync+cold --rw=randrw --size=1GB --end_fsync=1 --invalidate=1
</code></pre></div></div>

<p><img src="/assets/img/fio_randrw_fsync+cold.png" alt="fio_randrw_fsync+cold" /></p>

<p>It’s also worthwhile to simulate workloads that benefit from caching and
doesn’t require hard guarantees on disk writes since some applications
fall in this category.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  fio --name=expect --rw=randrw --size=1GB --pre_read=1
</code></pre></div></div>

<p><img src="/assets/img/fio_randrw.png" alt="fio_randrw" /></p>

<h3 id="workload-expectations">Workload expectations</h3>

<p>The above results measure the raw IO throughput to disk that will serve as
our upper bound. We’ll now use <code class="language-plaintext highlighter-rouge">hyperfine</code> against the system <code class="language-plaintext highlighter-rouge">cp -r</code>
command on a synthetic filesystem to get a baseline for the algorithm. The
filesystem properties are:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Directory depth: 5
Files per directory: 7
Filesize: 4k
Num files: 97655
Total size of IO = 480MB (R) + 480 (W) = 960MB
</code></pre></div></div>

<p><img src="/assets/img/cp_r_4k_fsync+cold.png" alt="cp_r_4k_fsync+cold" /></p>

<p><img src="/assets/img/cp_r_4k.png" alt="cp_r_4k" /></p>

<p>Our estimated throughput for <code class="language-plaintext highlighter-rouge">cp -r</code> is therefore:</p>

<p><strong>fsync+cold cache</strong>: 960MB / 30.421 = ~32MB/s</p>

<p><strong>regular</strong>: 960MB / 2.910s = ~330MB/s</p>

<p>We’re right around the ballpark of the results of the fio workload.</p>

<h2 id="can-we-do-better">Can we do better?</h2>

<p>Looking into how <code class="language-plaintext highlighter-rouge">cp</code> is implementation, it doesn’t employ any concurrency
and just performs a sequential walk through the directory using a blocking
syscalls. Remembering that read/writes are fast due to readahead and write
buffering, we’d guess that the sequential version may do better than the
concurrent version on workloads without “fsync + cold cache”. This is
because in actuality it spends little time waiting. With that
configuration on however, the sequential version starts suffers from long
suspension and the concurrent one really shines</p>

<h3 id="copy-algorithm-in-eio">Copy algorithm in eio</h3>

<p>Let’s look at the following algorithm taken from that post</p>

<div class="language-ocaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">open</span> <span class="nc">Eio</span>

<span class="k">let</span> <span class="p">(</span> <span class="o">/</span> <span class="p">)</span> <span class="o">=</span> <span class="nn">Eio</span><span class="p">.</span><span class="nn">Path</span><span class="p">.(</span> <span class="o">/</span> <span class="p">)</span>

<span class="k">let</span> <span class="n">copy</span> <span class="n">src</span> <span class="n">dst</span> <span class="o">=</span>
  <span class="k">let</span> <span class="k">rec</span> <span class="n">dfs</span> <span class="o">~</span><span class="n">src</span> <span class="o">~</span><span class="n">dst</span> <span class="o">=</span>
    <span class="k">let</span> <span class="n">stat</span> <span class="o">=</span> <span class="nn">Path</span><span class="p">.</span><span class="n">stat</span> <span class="o">~</span><span class="n">follow</span><span class="o">:</span><span class="bp">false</span> <span class="n">src</span> <span class="k">in</span>
    <span class="k">match</span> <span class="n">stat</span><span class="o">.</span><span class="n">kind</span> <span class="k">with</span>
    <span class="o">|</span> <span class="nt">`Directory</span> <span class="o">-&gt;</span>
      <span class="nn">Path</span><span class="p">.</span><span class="n">mkdir</span> <span class="o">~</span><span class="n">perm</span><span class="o">:</span><span class="n">stat</span><span class="o">.</span><span class="n">perm</span> <span class="n">dst</span><span class="p">;</span>
      <span class="k">let</span> <span class="n">files</span> <span class="o">=</span> <span class="nn">Path</span><span class="p">.</span><span class="n">read_dir</span> <span class="n">src</span> <span class="k">in</span>
      <span class="nn">List</span><span class="p">.</span><span class="n">iter</span> <span class="p">(</span><span class="k">fun</span> <span class="n">basename</span> <span class="o">-&gt;</span> <span class="n">dfs</span> <span class="o">~</span><span class="n">src</span><span class="o">:</span><span class="p">(</span><span class="n">src</span> <span class="o">/</span> <span class="n">basename</span><span class="p">)</span> <span class="o">~</span><span class="n">dst</span><span class="o">:</span><span class="p">(</span><span class="n">dst</span> <span class="o">/</span> <span class="n">basename</span><span class="p">))</span> <span class="n">files</span>
    <span class="o">|</span> <span class="nt">`Regular_file</span> <span class="o">-&gt;</span>
      <span class="nn">Path</span><span class="p">.</span><span class="n">with_open_in</span> <span class="n">src</span> <span class="o">@@</span> <span class="k">fun</span> <span class="n">source</span> <span class="o">-&gt;</span>
      <span class="nn">Path</span><span class="p">.</span><span class="n">with_open_out</span> <span class="o">~</span><span class="n">create</span><span class="o">:</span><span class="p">(</span><span class="nt">`Exclusive</span> <span class="n">stat</span><span class="o">.</span><span class="n">perm</span><span class="p">)</span> <span class="n">dst</span> <span class="o">@@</span> <span class="k">fun</span> <span class="n">sink</span> <span class="o">-&gt;</span>
      <span class="nn">Flow</span><span class="p">.</span><span class="n">copy</span> <span class="n">source</span> <span class="n">sink</span><span class="p">;</span>
    <span class="o">|</span> <span class="n">_</span> <span class="o">-&gt;</span> <span class="n">failwith</span> <span class="s2">"file type error"</span>
    <span class="k">in</span>
  <span class="n">dfs</span> <span class="o">~</span><span class="n">src</span> <span class="o">~</span><span class="n">dst</span>

<span class="k">let</span> <span class="bp">()</span> <span class="o">=</span>
  <span class="nn">Eio_linux</span><span class="p">.</span><span class="n">run</span> <span class="p">(</span><span class="k">fun</span> <span class="n">env</span> <span class="o">-&gt;</span>
      <span class="k">let</span> <span class="n">cwd</span> <span class="o">=</span> <span class="nn">Eio</span><span class="p">.</span><span class="nn">Stdenv</span><span class="p">.</span><span class="n">cwd</span> <span class="n">env</span> <span class="k">in</span>
      <span class="k">let</span> <span class="n">src</span> <span class="o">=</span> <span class="n">cwd</span> <span class="o">/</span> <span class="nn">Sys</span><span class="p">.</span><span class="n">argv</span><span class="o">.</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="k">in</span>
      <span class="k">let</span> <span class="n">dst</span> <span class="o">=</span> <span class="n">cwd</span> <span class="o">/</span> <span class="nn">Sys</span><span class="p">.</span><span class="n">argv</span><span class="o">.</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span> <span class="k">in</span>
      <span class="n">copy</span> <span class="n">src</span> <span class="n">dst</span><span class="p">)</span>
</code></pre></div></div>

<p>The above implementation is a minimal sequential version of <code class="language-plaintext highlighter-rouge">cp -r</code> using
eio’s API’s. However, right now we don’t expect to do better and in fact,
a quick time shows that this implementation takes <strong>8.8</strong> seconds to
complete, the throughput is only <strong>109MB/s</strong>. 3 times slower than the
regular <code class="language-plaintext highlighter-rouge">cp</code>. We will rack up this extra overhead as the cost of running
the concurrency infrastructure in a sequential manner for now.</p>

<p>To make this program concurrent, the simple change would be to spawn new
fibers to handle each file to copy.</p>

<div class="language-diff highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gd">- List.iter (fun ...)
</span><span class="gi">+ Fiber.List.iter ~max_fibers:2 (fun ...)
</span></code></pre></div></div>

<p>Though the problem with this method is that we have not much control on
the total number of fibers spawned. This is problematic for two reasons.
The first is that fibers have their own contexts and thus maintaining it
is extra memory space. Secondly, there is a limit on the number of open
file descriptors a process is allowed to have open. Looking at this
program, it is implemented with DFS. If the filesystem we are attempting
to copy is big enough, we could easily burst the limit on the number of
open file descriptors. It’s therefore neccessary to throttle the number of
fibers we spawn.</p>

<h3 id="copy-revised">Copy revised</h3>

<p>Our new version is structured using BFS in order to prevent holding file
descriptors open during the recursion in DFS. Additionally, it also uses
a semaphore to control the total number of fibers the program can spawn at
any time.</p>

<div class="language-ocaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">module</span> <span class="nc">Q</span> <span class="o">=</span> <span class="nn">Eio_utils</span><span class="p">.</span><span class="nc">Lf_queue</span>

<span class="k">let</span> <span class="n">copy_bfs</span> <span class="n">src</span> <span class="n">dst</span> <span class="o">=</span>

  <span class="k">let</span> <span class="n">sem</span> <span class="o">=</span> <span class="nn">Semaphore</span><span class="p">.</span><span class="n">make</span> <span class="mi">64</span> <span class="k">in</span>
  <span class="k">let</span> <span class="n">q</span> <span class="o">=</span> <span class="nn">Q</span><span class="p">.</span><span class="n">create</span> <span class="bp">()</span> <span class="k">in</span>
  <span class="nn">Q</span><span class="p">.</span><span class="n">push</span> <span class="n">q</span> <span class="p">(</span><span class="n">src</span><span class="o">,</span> <span class="n">dst</span><span class="p">);</span>

  <span class="nn">Switch</span><span class="p">.</span><span class="n">run</span> <span class="o">@@</span> <span class="k">fun</span> <span class="n">sw</span> <span class="o">-&gt;</span>
  <span class="k">while</span> <span class="n">not</span> <span class="p">(</span><span class="nn">Q</span><span class="p">.</span><span class="n">is_empty</span> <span class="n">q</span><span class="p">)</span> <span class="k">do</span>

    <span class="k">match</span> <span class="nn">Q</span><span class="p">.</span><span class="n">pop</span> <span class="n">q</span> <span class="k">with</span>
    <span class="o">|</span> <span class="nc">None</span> <span class="o">-&gt;</span> <span class="n">failwith</span> <span class="s2">"None in queue"</span>
    <span class="o">|</span> <span class="nc">Some</span> <span class="p">(</span><span class="n">src_path</span><span class="o">,</span> <span class="n">dst_path</span><span class="p">)</span> <span class="o">-&gt;</span>
      <span class="k">begin</span>
        <span class="k">let</span> <span class="n">stat</span> <span class="o">=</span> <span class="nn">Path</span><span class="p">.</span><span class="n">stat</span> <span class="o">~</span><span class="n">follow</span><span class="o">:</span><span class="bp">false</span> <span class="n">src_path</span> <span class="k">in</span>
        <span class="p">(</span><span class="k">match</span> <span class="n">stat</span><span class="o">.</span><span class="n">kind</span> <span class="k">with</span>
         <span class="o">|</span> <span class="nt">`Directory</span> <span class="o">-&gt;</span>
           <span class="nn">Path</span><span class="p">.</span><span class="n">mkdir</span> <span class="o">~</span><span class="n">perm</span><span class="o">:</span><span class="n">stat</span><span class="o">.</span><span class="n">perm</span> <span class="n">dst_path</span><span class="p">;</span>
           <span class="k">let</span> <span class="n">files</span> <span class="o">=</span> <span class="nn">Path</span><span class="p">.</span><span class="n">read_dir</span> <span class="n">src_path</span> <span class="k">in</span>
           <span class="c">(* Append files in found directory *)</span>
           <span class="nn">List</span><span class="p">.</span><span class="n">iter</span> <span class="p">(</span><span class="k">fun</span> <span class="n">f</span> <span class="o">-&gt;</span> <span class="nn">Q</span><span class="p">.</span><span class="n">push</span> <span class="n">q</span> <span class="p">(</span><span class="n">src_path</span> <span class="o">/</span> <span class="n">f</span><span class="o">,</span> <span class="n">dst_path</span> <span class="o">/</span> <span class="n">f</span><span class="p">))</span> <span class="n">files</span>
         <span class="o">|</span> <span class="nt">`Regular_file</span> <span class="o">-&gt;</span>
           <span class="nn">Fiber</span><span class="p">.</span><span class="n">fork</span> <span class="o">~</span><span class="n">sw</span> <span class="p">(</span><span class="k">fun</span> <span class="bp">()</span> <span class="o">-&gt;</span>
               <span class="nn">Semaphore</span><span class="p">.</span><span class="n">acquire</span> <span class="n">sem</span><span class="p">;</span>
               <span class="nn">Path</span><span class="p">.</span><span class="n">with_open_in</span> <span class="n">src_path</span> <span class="o">@@</span> <span class="k">fun</span> <span class="n">source</span> <span class="o">-&gt;</span>
               <span class="nn">Path</span><span class="p">.</span><span class="n">with_open_out</span> <span class="o">~</span><span class="n">create</span><span class="o">:</span><span class="p">(</span><span class="nt">`Exclusive</span> <span class="n">stat</span><span class="o">.</span><span class="n">perm</span><span class="p">)</span> <span class="n">dst_path</span> <span class="o">@@</span> <span class="k">fun</span> <span class="n">sink</span> <span class="o">-&gt;</span>
               <span class="nn">Flow</span><span class="p">.</span><span class="n">copy</span> <span class="n">source</span> <span class="n">sink</span><span class="p">;</span>
               <span class="nn">Semaphore</span><span class="p">.</span><span class="n">release</span> <span class="n">sem</span>
             <span class="p">)</span>
         <span class="o">|</span> <span class="n">_</span> <span class="o">-&gt;</span> <span class="n">failwith</span> <span class="s2">"Not sure how to handle kind"</span><span class="p">);</span>
        <span class="k">end</span>

  <span class="k">done</span>
</code></pre></div></div>

<h2 id="results">Results</h2>

<h3 id="benchmark-1">Benchmark 1</h3>

<p><code class="language-plaintext highlighter-rouge">eio_cp</code> against system <code class="language-plaintext highlighter-rouge">cp -r</code> with and without “fsync + cold cache”</p>

<p><img src="/assets/img/benchmark_1.png" alt="Benchmark1" /></p>

<p>Our results shows what we predicted, the eio version outperforms the sequential
one under “fsync + cold cache” configuration because it makes blocking occur
much more often.</p>

<p>Does that mean that if our workload benefits from having data in the page cache
and/or does not need strict persistence guarantees, we should favour the
sequential version? Well not quite. The filesystem we’ve been testing on has
many small files, making it much more likely that data can be found in cache
and each read/write returns quickly. Let’s see what happens when we increase
the size of files to <strong>1 MB</strong></p>

<h3 id="benchmark-2">Benchmark 2</h3>

<p>Our new test directory has the following structure:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Directory depth: 5
Files per directory: 4
Filesize: 1MB Num files: ... Total IO size = 780MB (R) + 780MB (W)
Num files: 780
Total IO size = 780MB (R) + 780MB (W) = 1560MB
</code></pre></div></div>
<p><img src="/assets/img/benchmark_2.png" alt="Benchmark2" /></p>

<p>Two things jump out at me. The first is that we’re noticing some
incredibly fast speeds, way above our expectations! The second is that we
are still behind the sequential version.</p>

<p>Initially I was puzzled by this result thinking that it was a bug in the
benchmark suite. After looking over it several times, I finally figured
out that it was because the workload change had rendered our theoretical
expectations entirely inaccurate. Reconsidering our workload, copying many
small files versus a few large is procedurally just much more stressful
for disks even if the total size of IO is the same. In the small files
case, the program has to do many iterations of opening, reading, writing.
Hence, this pattern is synonymous with the random mixed read write speed
we looked up earlier and is also the worst case scenario when it comes to
disk performance. On the flip side, by increasing the size of the files,
we’ve now altered this relationship to dealing with a much bigger block of
sequential reads and writes. Updating our expectations we rerun the fio
benchmark but now using a larger block size (The default previously was
4096).</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  fio --name=expect_upd_fsync+cold --rw=rw --size=1Gb --blocksize=1mi
  --end_fsync=1 --invalidate=1
</code></pre></div></div>

<p><img src="/assets/img/fio_rw_fsync+cold.png" alt="fio_rw_fsync+cold" /></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  fio --name=expect_upd --rw=rw --size=1GB --blocksize=1mi --pre_read=1
</code></pre></div></div>

<p><img src="/assets/img/fio_rw.png" alt="fio_rw" /></p>

<p>That now seems to make sense but we haven’t figured out why the concurrent
version still lags behind the sequential one. <code class="language-plaintext highlighter-rouge">eio</code> provides a useful
tracer <code class="language-plaintext highlighter-rouge">eio-trace</code> to visualize how fibers are interleaving in the
program. The trace produces</p>

<p><img src="/assets/img/eio-trace.png" alt="eio-trace" /></p>

<p>Hah! It looks like for every copy, there’s a long read/write loop.
Internally, eio uses 4096 Bytes as the default buffer size to copy between
files. This explains why copying a large file requires so many reads and
writes. Thankfully, the <code class="language-plaintext highlighter-rouge">Eio_linux.run</code> main function provides us an
option to configure this.</p>

<div class="language-ocaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">val</span> <span class="n">run</span> <span class="o">:</span>
  <span class="o">?</span><span class="n">queue_depth</span><span class="o">:</span><span class="kt">int</span> <span class="o">-&gt;</span>
  <span class="o">?</span><span class="n">n_blocks</span><span class="o">:</span><span class="kt">int</span> <span class="o">-&gt;</span>
  <span class="o">?</span><span class="n">block_size</span><span class="o">:</span><span class="kt">int</span> <span class="o">-&gt;</span>
  <span class="o">?</span><span class="n">polling_timeout</span><span class="o">:</span><span class="kt">int</span> <span class="o">-&gt;</span>
  <span class="o">?</span><span class="n">fallback</span><span class="o">:</span><span class="p">([</span><span class="nt">`Msg</span> <span class="k">of</span> <span class="kt">string</span><span class="p">]</span> <span class="o">-&gt;</span> <span class="k">'</span><span class="n">a</span><span class="p">)</span> <span class="o">-&gt;</span>
  <span class="p">(</span><span class="n">stdenv</span> <span class="o">-&gt;</span> <span class="k">'</span><span class="n">a</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="k">'</span><span class="n">a</span>
</code></pre></div></div>

<p>The main one of interest is <code class="language-plaintext highlighter-rouge">?block_size</code>. Let’s graph the difference in
performance by varying the blocksize to see the effect.</p>

<h3 id="benchmark-3">Benchmark 3</h3>
<p><img src="/assets/img/benchmark_3.png" alt="Benchmark3" /></p>

<p>Now that is a pretty substantial performance improvement! All with just
one tweak, though it took some real understanding to get here. So it’s not
always the case that the sequential version will fare better under
<code class="language-plaintext highlighter-rouge">fsync+cold</code> conditions.</p>

<h2 id="conclusion">Conclusion</h2>

<p>That’s it! I hope that by stepping through this process, it has provided
some useful context to help you think about adding concurrency for your
filesystem workloads. Just remember that even if there are obvious
opportunities to add concurrency, it’s not always going to provide you
strict speedup. In all likelihood your OS is already doing something
smart. Though having certain requirements or workloads could quickly tip
the scales in favour of a concurrent approach.</p>]]></content><author><name>Koon Wen Lee</name></author><summary type="html"><![CDATA[Getting concurrency right is often tricky. Recently, I’ve been playing around with eio, an IO concurrency library for OCaml. In particular, I was inspired by a discussion post asking how to implement an efficient directory copy using eio. The post attracted many suggestions and I was curious to understand which optimizations benefits eio the most. Turns out, this problem was a one-way ticket down the rabbit hole… Here’s how it went!]]></summary></entry><entry><title type="html">Understanding blockchain</title><link href="https://koonwen.github.io/2024/01/24/understanding-blockchain.html" rel="alternate" type="text/html" title="Understanding blockchain" /><published>2024-01-24T03:50:00+00:00</published><updated>2024-01-24T03:50:00+00:00</updated><id>https://koonwen.github.io/2024/01/24/understanding-blockchain</id><content type="html" xml:base="https://koonwen.github.io/2024/01/24/understanding-blockchain.html"><![CDATA[<p>I’ve recently started reading about blockchain. To consolidate my
understanding, I’ve put together an executive summary about what this
technology is all about.</p>

<h2 id="the-problem">The Problem</h2>
<p>As always, we should start with the motivation for the technology. Do
we trust banks as middlemen for our purchases? Even if we do, it comes
back to that age old adage that we “shouldn’t place all our eggs in
one basket”. what should happen if that central authority goes down?
Now to rephrase the question more generally: can we have a trust-less
service that allows us to exchange value?  Initially, this seems like
an obvious contradiction because the whole point of the service is to
act as a trusted third-party in the transaction. With blockchain
however, we answer the question in the affirmative.</p>

<h2 id="overview-by-example">Overview by example</h2>
<p>Before diving into the technical under-workings of blockchain, it pays
to get a general sense of what it is based on. Let’s say Alice and Bob
are performing an exchange. Alice hands Bob $10 and now Alice is now
$10 poorer and Bob is $10 richer. No trust is required because the
money was physically handed over. This is not how we usually make
transactions in the modern day though. Today we have banks that keep
track of how much we own and perform the transaction by deducting or
adding to our balance. So if Alice transfers Bob $10, Alice balance
sheet gets reduced by $10 and Bob increases by $10, no physical money
needs to exchange hands. That’s simple enough. The problem with this
is that now we need to trust that the bank records this correctly and
doesn’t make a blunder in updating the records (And all other worries
we have about trusting central authorities). This type of transaction
is akin to if Alice and Bob had a friend Charles that keeps track of
who owes who. In this case Alice owes Bob $10. This is problematic if
Charles mistakes the amount that was owed, worse still he may be
biased to his good friend Alice.</p>

<p>Now enter the third option, Alice, Bob and Charles live in a small
town of about 100 people. Instead of having only Charles keep track of
the exchange, what if everyone in the town helped out by individually
keeping track of the transaction. In such a case, we no longer have to
have to rely on Charles as a fair and trustworthy third-party. We can
spread our trust amongst the other 97 people in the town. If a dispute
where to happen, we can rely on the majority to validate the
transaction details. This is the premise of blockchain, a way to
distribute the responsibility of book-keeping. Of course, whilst this
is ideal, it has multiple problems.</p>

<ul>
  <li>How do we secure transactions?</li>
  <li>How do we verify a transaction occurred?</li>
  <li>
    <p>How do we get people to take part?</p>
  </li>
  <li>How do we prevent people from requesting a transaction when they have insufficient funds?</li>
  <li>How do we agree on an order of transactions since everyone is bound to receive them in different orders?</li>
</ul>

<p>The first three are the meat of blockchain technology and can be
described fairly generically. I will tackle those here. The last two
will be addressed in later posts discussing specific types of
blockchains.</p>

<h3 id="forward">Forward</h3>
<p>What I initially confused when learning about blockchain was the
process of securing a transaction and verifying that a transaction
occurred. To give the general idea, securing a transaction is about
knowing who currently owns the money. Verifying a transaction is then
the process of adding it to the public transaction log to confirms the
exchange of ownership of the money. To put it in less technical terms,
securing a transaction would be as though Alice signs her name that
she approves a transaction to Bob for $10. Verifying this transaction
would be to actually upgrade this local agreement to the global ledger
such that the transaction is visible to everyone.</p>

<h3 id="how-do-we-secure-transactions">How do we secure transactions?</h3>
<p>In the physical world, it suffices to say that if the money is
physically in my possession, it is mine (unless I’ve stolen it of
course). However, when dealing with digital currency, ownership is not
so straightforward. It turns out that the way to keep track of
ownership is by keeping a log of the history of transactions. You
could think of this as if we had to write down the name of the person
we were giving the money to before we pass it to them. To make this
chain secure so that only the people who own the money can spend it,
the transaction log is a trail of <strong>cryptographically</strong> secure
signatures of the people who previously owned it until it’s current
owner. Cryptography in this sense, is used to make stealing or
masquerading as someone else computationally infeasible.</p>

<p>The technical term for this is called <strong>Asymmetric cryptography (Or
public key cryptography)</strong>. At a high level, this works by each user
have a pair of keys. A public one and a private one. The public one is
available to other users whilst the private one is kept a secret only
to the user. These pair of keys have a unique feature to them which is
that they can be used to digitally sign a message and make it close to
impossible to forge. Procedurally, signing and verifying looks like
the following:</p>

<div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Sign(Message, private_key) =&gt; Signature

Verify(Message, Signature, public_key) =&gt; True/False
</code></pre></div></div>

<p>As mentioned, the important feature of this encryption process is that
it is there is no known way to reverse-engineer it. Knowing how the
algorithm works doesn’t allow one to discover the private
key. Furthermore, a slight change in the message contents, causes
wildly different signature so there’s no relationship/pattern we can
use to generate a valid signature other than guessing and checking.</p>

<p>Hence we arrive at the term cryptocurrency. We can now see where and
why cryptography is applied to the digital currency.</p>

<h3 id="how-do-we-verify-transactions">How do we verify transactions?</h3>
<p>After a secure transaction is broadcasted, it ends up in a pool of
“pending transactions”. At this time, we introduce other actors in the
system known as “miners”. In our example, the minors are analogous to
anyone in the town. The job of the miner is to pick up these
transactions and then validate them so that they end up on the public
ledger. For this process to occur, miners must first complete a
computational “challenge/puzzle”. Upon completion, it awards them a
prize and the ability to append a new block (some set of pending
transactions) onto the chain. It should be noted that appending a
block onto the chain doesn’t yet guarantee permanence or legitimacy of
the action. Instead, nodes work on the policy of following the longest
chain. This means that even if a bad actor were to successfully solve
the challenge before other nodes, it would only be able to append a
single block. To keep up with it’s scam, it would have to continuously
outperform the rest of the miners to have it’s chain become
validated. Unless the bad actor has the computational power of more
than 50% of the system, it is proven that it is computationally
infeasible for a bad actor to invalidate the system.</p>

<h3 id="how-do-we-get-people-to-take-part">How do we get people to take part?</h3>
<p>A vital part to a functioning blockchain is that there are a
distributed set of machines called “nodes” that are working to
validate transactions. Similar to how we have people in the town
actively keeping track of transactions that happen. To reward the
nodes, a node is given with a small amount of the
cryptocurrencies. This keeps people/businesses incentivized to run
their nodes participating in the exchange. On top of that, some
transaction even offer transaction fees that goes to the nodes (known
as miners) that successfully validate their transaction.</p>

<h2 id="conclusion">Conclusion</h2>
<p>To come full circle, given my understanding of blockchain works, I
don’t think it’s accurate to say that it’s trust-less. I think it is
trust-less in so far as we think of it as having no central
authority. It still remains the fact that we need to have trust in the
<strong>majority</strong> of actors in the system.  I suppose the answer is still
fairly clear that it’s better to place our bets on the majority rather
than a single entity, no matter how trustworthy.</p>]]></content><author><name>Koon Wen Lee</name></author><summary type="html"><![CDATA[I’ve recently started reading about blockchain. To consolidate my understanding, I’ve put together an executive summary about what this technology is all about.]]></summary></entry><entry><title type="html">Drawing boundaries for the Xen ecosystem</title><link href="https://koonwen.github.io/2024/01/23/drawing-boundaries-for-the-xen-ecosystem.html" rel="alternate" type="text/html" title="Drawing boundaries for the Xen ecosystem" /><published>2024-01-23T05:01:00+00:00</published><updated>2024-01-23T05:01:00+00:00</updated><id>https://koonwen.github.io/2024/01/23/drawing-boundaries-for-the-xen-ecosystem</id><content type="html" xml:base="https://koonwen.github.io/2024/01/23/drawing-boundaries-for-the-xen-ecosystem.html"><![CDATA[<p>The Xen ecosystem is quite ironic. It’s main product is hypervisor
that cleanly separates hardware resources. However, when it comes to
the organization of it’s ecosystem though, the lines between tools and
terminology are blurred and complicated. My post today is my
understanding and attempt to draw boundaries to help myself and other
beginners compartmentalize parts of the ecosystem. I will detail the 4
big components: Xen hypervisor, Xen-project, Xenserver and XAPI.</p>

<h2 id="xen-hypervisor">Xen hypervisor</h2>
<p>Xen is the name of a type-1 hypervisor (Bare metal) which gives
enables virtualization of multiple operating system to run on the
<strong><em>same</em></strong> host machine. In the software stack, the Xen hypervisor can be
thought of as a custom kernel (excluding device drivers) which “shims”
in the functionality to separate and isolate the hardware. On top of
the hypervisor, there are regions known as “domains” which can be seen
as a container for each guest operating system. There is a special
domain called “dom0” aka the “Control domain” which holds contains a
Linux kernel that provides the device drivers for the host. It is a
privileged domain that is able to “talk” directly to the hypervisor
and instruct it to spin up or shutdown other guest operating
systems. The device drivers in dom0 are shared with the other guest
operating systems in their respective unprivileged domains, referred
to as “domU’s”.</p>

<p>The order in which the
components interface with one another are as follows:</p>
<div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>            Guest OS_1, Guest OS_2, GuestOS_N... (domU's)
                        Control Domain (dom0)
                            Xen Hypervisor
                          Physical Hardware
</code></pre></div></div>

<h2 id="xen-project">Xen-project</h2>
<p>The Xen-project is a collection of open source projects hosted under
the <strong><em>Linux foundation</em></strong>. It contains the hypervisor itself and
several sub-projects including: Windows PV driver, MirageOS, unikraft,
XAPI, XCP, etc.</p>

<h4 id="terminology-what-is-a-tool-stack">Terminology: What is a Tool stack?</h4>
<blockquote>
  <p>A tool stack is a set of cooperating daemons that provide a
higher-level interface to manage Xen hosts. It extends the base
hypervisor to provides system management tools such as basic setup,
configuration, etc.</p>
</blockquote>

<h2 id="xapi">XAPI</h2>
<p>XAPI refers to Xen-project default <strong>tool stack</strong> which includes basic
system management as well as functionality to remotely configuring and
controlling virtualised guests on hosts.</p>

<h2 id="xenserver">Xenserver</h2>
<p>An enterprise platform for orchestrating/managing Xen
virtualization. It packages the hypervisor, XAPI tool stack, custom
Linux kernel and other open source components (i.e. XenCenter ) to
provides a platform for large scale management. It can be thought of
as a “distro” because much of the work is about versioning and
integration of components. They also - like a distro, provide seamless
upstream patching for their users. It is being led by Citrix (now
acquired by Vista Equity Partners and Evergreen Coast Capital).</p>

<h2 id="diagram">Diagram</h2>
<p>I think the relationship between these entities are best described
visually by these concentric circles.</p>

<p><img src="/assets/img/xen-diagram.png" alt="diagram" /></p>

<h2 id="conclusion">Conclusion</h2>
<p>In essence, we have the hypervisor which is the lowest level software
interacting with the hardware - providing the virtualization
capabilities. On top of that, XAPI is a collection of tools that
address system management of virtual machines using Xen. It includes
more than just a regular tool stack with remote management of Xen
hosts. The Xen-project is the overarching entity of open-source
projects around Xen including the hypervisor, XAPI and other tools
extending Xen. Finally Xenserver is an enterprise platform that builds
around XAPI and other tools with orchestration capabilities
Sysadmin-like user interface (XenCenter). The work being done around
Xenserver is “distro-like” in it’s responsibilities of package
versioning and ensuring seamless integration between tools.</p>]]></content><author><name>Koon Wen Lee</name></author><summary type="html"><![CDATA[The Xen ecosystem is quite ironic. It’s main product is hypervisor that cleanly separates hardware resources. However, when it comes to the organization of it’s ecosystem though, the lines between tools and terminology are blurred and complicated. My post today is my understanding and attempt to draw boundaries to help myself and other beginners compartmentalize parts of the ecosystem. I will detail the 4 big components: Xen hypervisor, Xen-project, Xenserver and XAPI.]]></summary></entry><entry><title type="html">Disciplined indexing with Binary-search</title><link href="https://koonwen.github.io/2024/01/18/disciplined-indexing-with-binary-search.html" rel="alternate" type="text/html" title="Disciplined indexing with Binary-search" /><published>2024-01-18T12:33:00+00:00</published><updated>2024-01-18T12:33:00+00:00</updated><id>https://koonwen.github.io/2024/01/18/disciplined-indexing-with-binary-search</id><content type="html" xml:base="https://koonwen.github.io/2024/01/18/disciplined-indexing-with-binary-search.html"><![CDATA[<p>The topic for today’s article was churned up out of frustration thanks
to the classic “off-by-one” error. The especially annoying thing is
that Binary-search is one of those elementary algorithms that you
should be able to produce without hiccup. Except that it’s happened
now a couple times that I spend way too long figuring out where I’m
messing up the indices. With this article, I put forth a set of
“rules” to think about when implementing binary search in future.</p>

<h2 id="binary-search-a-summary">Binary Search: a summary</h2>
<p>Binary search is a quick way to search technique given an <strong>ordered
collection</strong>. The search runs by looking at the arbitrary midpoint and
checking if we should continue the search to the left or to the
right. We do this same process until we end up with just one element,
at which point we can decide that the search was successful or
not. Being able to halve the problem every time leads to a fantastic
time complexity of O(log(n)).</p>

<h2 id="bisect--binary-search">Bisect &gt; Binary search</h2>
<p>One thing I learnt about approaching Binary search problems, is that
you should think about them instead as a bisection problems. This
means that rather than viewing the algorithm as a search method, we
look at it from the vantage point of a method to reduce our search
space. This subtle shift in mind-set arrives at a different
perspective of our usual “We use Binary search to look for an element”
to “We use Binary search to throw away the bits that don’t matter”.</p>

<p>Still confused? Let’s put it into concrete terms. Given some sorted
array <code class="language-plaintext highlighter-rouge">arr</code> of elements, identify if the element <code class="language-plaintext highlighter-rouge">k</code> is in the
array. If we are thinking about this in terms of bisection, our
question is now: “identify the smallest index where the element is
equal to <code class="language-plaintext highlighter-rouge">k</code>”</p>

<p>Here are some example arrays where we’d expect the final index to land</p>
<div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>k = 3

Case1:[1,2,3,4,5,6]
           ^
Case2:[3,3,3,3,3,3]
       ^
Case3:[1,2,3,3,3,4]
           ^
Case4:[1,1,1,1,1,1]
                   ^
Case5:[4,4,4,4,4,4]
       ^
</code></pre></div></div>

<p>Cases1-3 are fairly self explanatory, we landed on the index where the
element under the index is equal to 3 <code class="language-plaintext highlighter-rouge">and</code> is also the earliest 3 we
encounter if we traverse the list from left to right. How about Case4?
What if 3 is not in the list? Then by our bisection criteria, we
should rightly land outside the list, Translated into indices, we only
have <code class="language-plaintext highlighter-rouge">0 - N-1</code> index elements, but we will allow our search to also
land on <code class="language-plaintext highlighter-rouge">N</code>. This means that if our index is on <code class="language-plaintext highlighter-rouge">N</code> we can conclude
that <code class="language-plaintext highlighter-rouge">k</code> is not in the array.</p>

<p>However, look at Case5. Although <code class="language-plaintext highlighter-rouge">3</code> is not in the list, the final
index lands on position <code class="language-plaintext highlighter-rouge">0</code> (In accordance to how expect the search to
move leftwards rather than to the right as in Case4). This is a
problem, shouldn’t we also allow <code class="language-plaintext highlighter-rouge">-1</code> to be a possible index to land
on? The answer is yes but mostly no. Yes, in that we should be landing
outside the array bounds following the bisection principle. No because
this will complicate how we decide to bisect the array as well as how
we find divide to find the midpoint. The workaround is to have two
checks at the end:</p>

<ul>
  <li>Make sure you’re within array bounds</li>
  <li>if you’re within, check that the element under my index is equal to
<code class="language-plaintext highlighter-rouge">k</code></li>
</ul>

<h2 id="termination">Termination</h2>
<p>Another thing to make sure to get right, is that the algorithm doesn’t
get stuck looping on the same index. The key to this are three things,
the <strong>loop invariant</strong>, how you find the <strong>midpoint</strong> and how you get the
new <strong>array bounds</strong>. These three <strong>MUST</strong> be compatible to ensure
that your algorithm will eventually hit the base case. One combination
that works is:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">loop</span> <span class="n">invariant</span><span class="p">:</span> <span class="n">lo</span> <span class="o">&lt;</span> <span class="n">hi</span>

<span class="n">midpoint</span><span class="p">:</span> <span class="p">(</span><span class="n">lo</span> <span class="o">+</span> <span class="n">hi</span><span class="p">)</span> <span class="o">//</span> <span class="mi">2</span>    <span class="c1"># floor division
</span>
<span class="n">array</span> <span class="n">bounds</span><span class="p">:</span> <span class="k">if</span> <span class="n">arr</span><span class="p">[</span><span class="n">midpoint</span><span class="p">]</span> <span class="o">==</span> <span class="n">k</span><span class="p">:</span>
                  <span class="n">hi</span> <span class="o">=</span> <span class="n">midpoint</span>
              <span class="k">else</span><span class="p">:</span>
                  <span class="n">lo</span> <span class="o">=</span> <span class="n">midpoint</span> <span class="o">+</span> <span class="mi">1</span>
</code></pre></div></div>

<p>Let’s go through why this work, starting with the array bounds. Given
our bisection criteria again, if the element we are currently on is
equal to <code class="language-plaintext highlighter-rouge">k</code>, we may or may not be on the smallest index. Eitherways
we have to keep it’s index <strong>inside</strong> the array bounds so we only
update <code class="language-plaintext highlighter-rouge">hi = midpoint</code>. In the <code class="language-plaintext highlighter-rouge">else</code> case, we know for certainty that
we can exclude the current index, therefore the ‘+1’.</p>

<p>For the midpoint calculation, we use floor division which gives us the
following properties. If the number of elements we are considering is
<code class="language-plaintext highlighter-rouge">odd</code> we have an true midpoint. Floor division gives the correct index
of that midpoint when we use indices bounds. E.g. <code class="language-plaintext highlighter-rouge">(0 + 4) // 2 = 2</code>.
<strong>index 2</strong> in the <code class="language-plaintext highlighter-rouge">[1,2,3,4,5]</code> is the element 3, perfectly in the
middle. However, if we are performing the division for an even number
of elements, then we land to the left of the middle. So the index that
we land on for the array <code class="language-plaintext highlighter-rouge">[1,2,3,4]</code>, would be <code class="language-plaintext highlighter-rouge">(0+3) // 2 = 1</code>
<strong>index 1</strong>, which is <strong>element 2</strong>.</p>

<p>With the above constraints, the only way we can “get stuck” is if we
always calculate the same midpoint and the value under the midpoint
index is equal to <code class="language-plaintext highlighter-rouge">k</code>, so that <code class="language-plaintext highlighter-rouge">hi</code> remains the same. We can narrow
this down further to when <code class="language-plaintext highlighter-rouge">lo</code> and <code class="language-plaintext highlighter-rouge">hi</code> are next to each other since
our loop invariant exits when <code class="language-plaintext highlighter-rouge">lo == hi</code>. Now we just have to realize
that our previous argument said that if we are considering an even
number of elements, we always end up to the left of the middle. That
means that our <code class="language-plaintext highlighter-rouge">hi</code> will always progress to equal <code class="language-plaintext highlighter-rouge">lo</code> and terminate
the loop invariant. Nice!</p>

<h2 id="all-in-all">All in all</h2>
<p>Now why go through all that effort to turn the binary search into
bisection? For a lot of questions I’ve run into, binary search is
often employed to find bisection. E.g “Find the earliest occurrence in
a git history of a bad commit”. In such cases, I mess up by being
“one-off” because the search criteria in my head is looking for an
exact point. Trying to edit it after is like working out if the three
factors for termination play nice which is a disaster. Bisection works
in both cases so it’s much better to go with one disciplined approach.</p>

<p>Our final solution looks something like this that can be modified
based on the specific algorithm</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">bin_search</span><span class="p">(</span><span class="n">arr</span><span class="p">,</span> <span class="n">k</span><span class="p">):</span>
    <span class="n">lo</span><span class="p">,</span> <span class="n">hi</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">hi</span><span class="p">)</span>   <span class="c1"># We include the len(hi) index as discussed earlier
</span>    <span class="k">while</span> <span class="n">lo</span> <span class="o">&lt;</span> <span class="n">hi</span><span class="p">:</span>
        <span class="n">midpoint</span> <span class="o">=</span> <span class="p">(</span><span class="n">lo</span> <span class="o">+</span> <span class="n">hi</span><span class="p">)</span> <span class="o">//</span> <span class="mi">2</span>
        <span class="k">if</span> <span class="n">arr</span><span class="p">[</span><span class="n">midpoint</span><span class="p">]</span> <span class="o">==</span> <span class="n">k</span><span class="p">:</span>
            <span class="n">hi</span> <span class="o">=</span> <span class="n">midpoint</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">lo</span> <span class="o">=</span> <span class="n">midpoint</span> <span class="o">+</span> <span class="mi">1</span>
    <span class="k">return</span> <span class="n">lo</span> <span class="o">&lt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">arr</span><span class="p">)</span> <span class="ow">and</span> <span class="n">arr</span><span class="p">[</span><span class="n">lo</span><span class="p">]</span> <span class="o">==</span> <span class="n">k</span>
</code></pre></div></div>

<p>Checking either <code class="language-plaintext highlighter-rouge">lo</code> or <code class="language-plaintext highlighter-rouge">hi</code> will work because it is always the case
that they will be equal to each other.</p>

<p>Of course there are different ways to design this focusing on
different bisection criteria. But since I’ve gone through the effort
to prove to myself that this combination works, I’ll take this as my
template solution.</p>

<h5 id="a-lot-of-my-ideas-here-are-inspired-by-this-great-article">A lot of my ideas here are inspired by this great <a href="http://coldattic.info/post/95/">article</a>.</h5>]]></content><author><name>Koon Wen Lee</name></author><summary type="html"><![CDATA[The topic for today’s article was churned up out of frustration thanks to the classic “off-by-one” error. The especially annoying thing is that Binary-search is one of those elementary algorithms that you should be able to produce without hiccup. Except that it’s happened now a couple times that I spend way too long figuring out where I’m messing up the indices. With this article, I put forth a set of “rules” to think about when implementing binary search in future.]]></summary></entry><entry><title type="html">Breaking down Dynamic-Programming</title><link href="https://koonwen.github.io/2024/01/15/breaking-down-dynamic-programming.html" rel="alternate" type="text/html" title="Breaking down Dynamic-Programming" /><published>2024-01-15T15:22:00+00:00</published><updated>2024-01-15T15:22:00+00:00</updated><id>https://koonwen.github.io/2024/01/15/breaking-down-dynamic-programming</id><content type="html" xml:base="https://koonwen.github.io/2024/01/15/breaking-down-dynamic-programming.html"><![CDATA[<p>Dynamic-Programming (DP) is one of those things that makes my brain
overheat. To a first approximation, DP is an algorithmic trick that
improves the time complexity of otherwise exponential algorithms. DP
manages this because it avoids doing unnecessary re-computation - A
common feature that causes problems to grow exponentially. Instead
of re-computing something we’ve already done before, DP says “let’s
store this result because I’ll have to refer back to it”.  Sound
simple? In practice, not so much. This post tries to provide a clear
step by step to approach such problems.</p>

<h2 id="when-does-dp-apply">When does DP apply?</h2>
<p>The first thing that makes DP challenging is that it’s not clear when
it applies. I believe there’s no simple way to figure it out
either. So far, I use some heuristics like thinking about if the
problem asks for “an optimal combination”. This usually means that the
brute force solution would have you calculate <em>all possibilities</em> to
find the best one. This is the dead giveaway that your algorithm
diverges. That said, sometimes problems that seem like DP could
actually just by a simple Greedy algorithm, so it’s worth trying that
approach first before committing to DP.</p>

<h2 id="a-problem">A Problem</h2>
<p>Calculate the i-th term Fibonacci sequence</p>
<pre><code class="language-math">Fib(n) = Fib(n-1) + Fib(n-2)
</code></pre>

<h3 id="naive-solution">Naive solution</h3>
<p>This is pretty much in your face exponential because of the recursive
definition. Implementing it recursively is basically brute force and
would yield exponential time complexity</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">fib</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">n</span> <span class="o">&lt;=</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">return</span> <span class="mi">1</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">fib</span><span class="p">(</span><span class="n">n</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">fib</span><span class="p">(</span><span class="n">n</span><span class="o">-</span><span class="mi">2</span><span class="p">)</span></code></pre></figure>

<h3 id="looking-top-down">Looking Top-down</h3>
<p>The problem with our recursive solution is that we have a lot of
branching and incur many repeated sub-computation.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>              fib(4)
             /      \
        fib(3)       fib(2)
        /   \        /    \
    fib(2)  fib(1) fib(1) fib(0)
    /    \
fib(1)  fib(0)

We can see that *fib(2)* is repeated 2 times and fib(1) and
fib(0), 3 and 2 times respectively.
</code></pre></div></div>

<p>One way to visualize your algorithm is to think if the evaluation
expands into a N-branch tree. In that case, you can start think if you
can leverage <strong>Memoization</strong>. Memoization is the one of two DP
approaches. As per the name, here the memo is where we are going to
save the result of some sub-computation and lookup later when we need
it. How we’ll do this is to add a table lookup just before we descend
into the recursion and make sure to save the results into the table
when we get them.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">memo</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">fib</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">n</span> <span class="ow">in</span> <span class="n">memo</span><span class="p">:</span> <span class="k">return</span> <span class="n">memo</span><span class="p">[</span><span class="n">n</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">n</span> <span class="o">&lt;=</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">return</span> <span class="mi">1</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">res</span> <span class="o">=</span> <span class="n">fib</span><span class="p">(</span><span class="n">n</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">fib</span><span class="p">(</span><span class="n">n</span><span class="o">-</span><span class="mi">2</span><span class="p">)</span>
        <span class="n">memo</span><span class="p">[</span><span class="n">n</span><span class="p">]</span> <span class="o">=</span> <span class="n">res</span>
        <span class="k">return</span> <span class="n">res</span></code></pre></figure>

<p>If you can’t yet tell, this basically causes the recursion to
short-circuit because it can find the previously computed results in
the table. Our runtime is significantly better since we only really
descend down the left side of the tree and every subsequent right
child’s result cost a single constant lookup. Something to note about
the time complexity is that this is O(n), not O(log(n)). You might be
quick to assume that because of the tree I drew but actually we
descend n times down the left side, so we haven’t split the problem in
half.</p>

<blockquote>
  <p>We call this a top-down approach because the recursive call begins
at the top and bottoms out at the base case and builds back up the
result.</p>
</blockquote>

<h3 id="from-bottom-up">From Bottom-up</h3>
<p>The second technique DP technique is called <strong>Tabulation</strong>. With this
method, we skip the recursion downward and just start directly from
the base case and build the result up. We also use a table to keep
track and reference our previous results. Our Fibonacci problem is not
the most motivating case for a table (explained later) but I will
demonstrate it anyway. For intuition, instead of trying to calculate
our results from the top, notice that working from the bottom pretty
easy. That is, we can find fib(2) easily because it is fib(1) + fib(0)
which we know to be 1 + 1 effectively. Then fib(3) is fib(2) + fib(1)
and we just calculated the result of fib(2), and so forth.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">fib</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
    <span class="n">tbl</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">n</span>
    <span class="n">tbl</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">tbl</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">n</span><span class="p">):</span>
        <span class="n">tbl</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">tbl</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">tbl</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">tbl</span><span class="p">[</span><span class="n">n</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
</code></pre></div></div>

<p>In this implementation, we iteratively calculate up toward the value
that we want using the result of previous calculated
result. Leveraging the same idea used in memoization, just in a
different style.</p>

<blockquote>
  <p>This is bottom-up because we start directly from the base case. An
interesting note is that memoization usually involves recursion and
tabulation uses simple iteration so it is generally faster because
it doesn’t need to allocate stack frames for the recursive function
calls.</p>
</blockquote>

<h2 id="a-harder-problem">A Harder Problem</h2>
<p>As alluded to, using tabulation for the Fibonnacci problem is
unnecessary here because we only really need to keep track of one
variable to calculate the subsequent result. Tabulation shines when
there are multiple “choices” to be made. Let’s see an example:</p>

<p>“The assembly-line problem: Given 2 assembly lines, each with M
stations where M is some given integer value representing the time
taken at the station. Determine the shortest path that can be taken
through the factory if there is also a cost to transferring between
assembly lines”</p>

<div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Line1: 5 -&gt; 2  -&gt; 10 -&gt; 7
Line2: 1 -&gt; 12 -&gt; 1  -&gt; 1

Line1 to Line2 transfer cost: [2, 1,  5]
Line2 to Line1 transfer cost: [1, 12, 15]

The optimal solution is the following:
Line1:    5    2   10   7
             /   \
         +1 /     \ +1
           /       \
Line2: -&gt; 1    12    1 -&gt; 1  =&gt; 1 + 1 + 2 + 1 + 1 + 1 = 7
</code></pre></div></div>

<p>It’s worth looking first what the recursive solution might look like</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">path</span><span class="p">(</span><span class="n">l1</span><span class="p">,</span> <span class="n">l2</span><span class="p">,</span> <span class="n">l1_l2</span><span class="p">,</span> <span class="n">l2_l1</span><span class="p">):</span>
    <span class="n">length</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">l1</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">aux</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">line</span><span class="p">,</span> <span class="n">line_other</span><span class="p">,</span> <span class="n">switch</span><span class="p">,</span> <span class="n">switch_other</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">line</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">elif</span> <span class="n">i</span> <span class="o">==</span> <span class="n">length</span><span class="p">:</span>
            <span class="k">return</span> <span class="nb">min</span><span class="p">(</span><span class="n">aux</span><span class="p">(</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">l1</span><span class="p">,</span> <span class="n">l2</span><span class="p">,</span> <span class="n">l2_l1</span><span class="p">,</span> <span class="n">l1_l2</span><span class="p">),</span> <span class="n">aux</span><span class="p">(</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">l2</span><span class="p">,</span> <span class="n">l1</span><span class="p">,</span> <span class="n">l1_l2</span><span class="p">,</span> <span class="n">l2_l1</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">stay</span> <span class="o">=</span> <span class="n">aux</span><span class="p">(</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">line</span><span class="p">,</span> <span class="n">line_other</span><span class="p">,</span> <span class="n">switch</span><span class="p">,</span> <span class="n">switch_other</span><span class="p">)</span>
            <span class="n">switch</span> <span class="o">=</span> <span class="n">aux</span><span class="p">(</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">line_other</span><span class="p">,</span> <span class="n">line</span><span class="p">,</span> <span class="n">switch_other</span><span class="p">,</span> <span class="n">switch</span><span class="p">)</span> <span class="o">+</span> <span class="n">switch</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
            <span class="k">return</span> <span class="nb">min</span><span class="p">(</span><span class="n">stay</span><span class="p">,</span> <span class="n">switch</span><span class="p">)</span> <span class="o">+</span> <span class="n">line</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">aux</span><span class="p">(</span><span class="n">length</span><span class="p">,</span> <span class="n">l1</span><span class="p">,</span> <span class="n">l2</span><span class="p">,</span> <span class="n">l2_l1</span><span class="p">,</span> <span class="n">l1_l2</span><span class="p">)</span>
</code></pre></div></div>

<p>As you can see, we have to keep track of multiple variables in our
<code class="language-plaintext highlighter-rouge">aux</code> recursive function which makes the code rather clunky. Adding in
memoization would just make it even more complex. Now look at the
tabulation solution</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">path_tab</span><span class="p">(</span><span class="n">l1</span><span class="p">,</span> <span class="n">l2</span><span class="p">,</span> <span class="n">l1_l2</span><span class="p">,</span> <span class="n">l2_l1</span><span class="p">):</span>
    <span class="n">length</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">l1</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">length</span><span class="p">):</span>
        <span class="n">l1</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+=</span> <span class="nb">min</span><span class="p">(</span><span class="n">l1</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">l2</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">l2_l1</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
        <span class="n">l2</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+=</span> <span class="nb">min</span><span class="p">(</span><span class="n">l2</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">l1</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">l1_l2</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
    <span class="k">return</span> <span class="nb">min</span><span class="p">(</span><span class="n">l1</span><span class="p">[</span><span class="n">length</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">l2</span><span class="p">[</span><span class="n">length</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
</code></pre></div></div>

<p>The iterative solution to my mind is much easier to follow what is
going and also overall cleaner. Unlike the recursive solution, we
don’t have to pass on variables to the next recursive call.</p>

<h2 id="conclusion">Conclusion</h2>
<p>This sums up the introduction to Dynamic programming, why it works,
when it applies and the trade-off between the two main techniques.</p>

<blockquote>
  <p>When you see branching with similar looking sub-branches, it’s
probably going to need DP.</p>
</blockquote>]]></content><author><name>Koon Wen Lee</name></author><summary type="html"><![CDATA[Dynamic-Programming (DP) is one of those things that makes my brain overheat. To a first approximation, DP is an algorithmic trick that improves the time complexity of otherwise exponential algorithms. DP manages this because it avoids doing unnecessary re-computation - A common feature that causes problems to grow exponentially. Instead of re-computing something we’ve already done before, DP says “let’s store this result because I’ll have to refer back to it”. Sound simple? In practice, not so much. This post tries to provide a clear step by step to approach such problems.]]></summary></entry><entry><title type="html">Continuation-Passing-Style for the pragmatic layman</title><link href="https://koonwen.github.io/2024/01/07/continuation-passing-style-for-the-pragmatic-layman.html" rel="alternate" type="text/html" title="Continuation-Passing-Style for the pragmatic layman" /><published>2024-01-07T08:47:00+00:00</published><updated>2024-01-07T08:47:00+00:00</updated><id>https://koonwen.github.io/2024/01/07/continuation-passing-style-for-the-pragmatic-layman</id><content type="html" xml:base="https://koonwen.github.io/2024/01/07/continuation-passing-style-for-the-pragmatic-layman.html"><![CDATA[<p>I’ve always found continuation-passing-style (CPS) one of the more an
elusive concept to grasp. Today I came across a simple tree traversal
problem that helped me work through some of that complexity.</p>

<h2 id="the-problem">The Problem</h2>
<p>Suppose you are given an integer value <code class="language-plaintext highlighter-rouge">target</code> and a binary tree
whereby each node has a integer value assigned to it. Identify if a
path exists from the root to leaf whereby the total sum of the path
equates to <code class="language-plaintext highlighter-rouge">target</code>.</p>

<h3 id="solution-1-classic-recursion">Solution 1: Classic recursion</h3>
<p>At first glance, this question can be solved with the classic
recursive approach. That is, we recursively traverse down the tree
until we hit a leaf node as our base case. Along the way, we subtract
the value of the current node from the <code class="language-plaintext highlighter-rouge">target</code> and pass on the
difference. In the base case, we now have the result whether the sum
of the path is equal to the <code class="language-plaintext highlighter-rouge">target</code> and we will need to bubble up the
result. In OCaml, we would have something like</p>

<figure class="highlight"><pre><code class="language-ocaml" data-lang="ocaml"><span class="k">type</span> <span class="k">'</span><span class="n">a</span> <span class="n">node</span> <span class="o">=</span>
  <span class="o">|</span> <span class="nc">Lf</span>
  <span class="o">|</span> <span class="nc">Br</span> <span class="k">of</span> <span class="k">'</span><span class="n">a</span> <span class="o">*</span> <span class="k">'</span><span class="n">a</span> <span class="n">node</span> <span class="o">*</span> <span class="k">'</span><span class="n">a</span> <span class="n">node</span>

<span class="k">let</span> <span class="k">rec</span> <span class="n">find_path</span> <span class="n">node</span> <span class="n">target</span> <span class="o">=</span>
  <span class="k">match</span> <span class="n">node</span> <span class="k">with</span>
  <span class="o">|</span> <span class="nc">Lf</span> <span class="o">-&gt;</span> <span class="n">target</span> <span class="o">=</span> <span class="mi">0</span>
  <span class="o">|</span> <span class="nc">Br</span> <span class="p">(</span><span class="n">v</span><span class="o">,</span> <span class="n">l</span><span class="o">,</span> <span class="n">r</span><span class="p">)</span> <span class="o">-&gt;</span>
      <span class="k">let</span> <span class="n">lv</span> <span class="o">=</span> <span class="n">find_path</span> <span class="n">l</span> <span class="p">(</span><span class="n">target</span> <span class="o">-</span> <span class="n">v</span><span class="p">)</span> <span class="k">in</span>
      <span class="k">let</span> <span class="n">rv</span> <span class="o">=</span> <span class="n">find_path</span> <span class="n">r</span> <span class="p">(</span><span class="n">target</span> <span class="o">-</span> <span class="n">v</span><span class="p">)</span> <span class="k">in</span>
      <span class="n">lv</span> <span class="o">||</span> <span class="n">rv</span></code></pre></figure>

<p>This works, but it’s very memory intensive. The recursive calls blow
up the stack with a lot of pending function frames. Can we do better?
More precisely, can we make this function tail-recursive?</p>

<p>Not all functions are created equal. Some happen to be easier to be
converted their tail-recursive counterparts. Unfortunately,
<code class="language-plaintext highlighter-rouge">find_path</code> is one of those difficult functions. The reasons why this
is the case currently escapes me and will be tackled in a later
post. For now, let’s assume that to be the case. (Or you can try
to come up with the tail-recursive solution yourself)</p>

<h3 id="solution-2-cps-transformation">Solution 2: CPS transformation</h3>
<p>Turns out, the way to convert such a function to a tail-recursive
version is to leverage the idea of <em>continuations</em>. There’s a lot of
technical literature on the topic obfuscating what the term refers
to. To my mind, it just means “what is next thing to do”. In other
words, if you ran a program and paused it at some arbitrary point
(think a debugger), what’s left to do is the continuation of that
program. So continuation-passing-style, is a way of programming where
we explicitly pass functions “the next thing to do”. What does that
look like in code? Instead of functions returning values to the
caller, we can design functions that return with a new function call.</p>

<figure class="highlight"><pre><code class="language-ocaml" data-lang="ocaml"><span class="k">let</span> <span class="n">find_path_cps</span> <span class="n">node</span> <span class="n">target</span> <span class="o">=</span>
  <span class="k">let</span> <span class="k">rec</span> <span class="n">aux</span> <span class="n">node</span> <span class="n">target</span> <span class="n">k</span> <span class="o">=</span>
    <span class="k">match</span> <span class="n">node</span> <span class="k">with</span>
    <span class="o">|</span> <span class="nc">Lf</span> <span class="o">-&gt;</span> <span class="n">k</span> <span class="p">(</span><span class="mi">0</span> <span class="o">=</span> <span class="n">target</span><span class="p">)</span>
    <span class="o">|</span> <span class="nc">Br</span> <span class="p">(</span><span class="n">v</span><span class="o">,</span> <span class="n">l</span><span class="o">,</span> <span class="n">r</span><span class="p">)</span> <span class="o">-&gt;</span>
        <span class="n">aux</span> <span class="n">l</span> <span class="p">(</span><span class="n">target</span> <span class="o">-</span> <span class="n">v</span><span class="p">)</span> <span class="p">(</span><span class="k">fun</span> <span class="n">lv</span> <span class="o">-&gt;</span>
            <span class="n">aux</span> <span class="n">r</span> <span class="p">(</span><span class="n">target</span> <span class="o">-</span> <span class="n">v</span><span class="p">)</span> <span class="p">(</span><span class="k">fun</span> <span class="n">rv</span> <span class="o">-&gt;</span>
                <span class="n">k</span> <span class="p">(</span><span class="n">lv</span> <span class="o">||</span> <span class="n">rv</span><span class="p">)))</span>
  <span class="k">in</span>
  <span class="n">aux</span> <span class="n">node</span> <span class="n">target</span> <span class="nn">Fun</span><span class="p">.</span><span class="n">id</span></code></pre></figure>

<p>This looks confusing and it is. I took some time to really process
what exactly is going on in this function. Let’s start with the most
obvious. Firstly our cps version defines an auxiliary function with an
added parameter <code class="language-plaintext highlighter-rouge">k</code> that stands for continuation. This parameter can
be thought of as a kind of accumulator that builds up the things “left
to do” in the same way that the recursive solution builds up stack
frames to keep track of pending computation to execute later. In CPS
we don’t leave anything in the stack pending instead by passing the
<em>continuation</em> represented by a function to the next recursive
call. In such a way, our cps inspired function is now tail-recursive
by passing around a function pointer.</p>

<h3 id="solution-3-short-circuiting-cps">Solution 3: Short-circuiting CPS</h3>
<p>Reducing memory usage is one of the main advantages of CPS but it also
allows us to “short-circuit” functions. Specifically for the case of
<code class="language-plaintext highlighter-rouge">find_path</code> we could design it in such a way to “forget” about what’s
left to do and have an escape hatch that allows us to return early
once we found a path that exists.</p>

<figure class="highlight"><pre><code class="language-ocaml" data-lang="ocaml"><span class="k">let</span> <span class="n">find_path_cps_fast</span> <span class="n">node</span> <span class="n">target</span> <span class="o">=</span>
  <span class="k">let</span> <span class="k">rec</span> <span class="n">aux</span> <span class="n">node</span> <span class="n">target</span> <span class="n">k</span> <span class="o">=</span>
    <span class="k">match</span> <span class="n">node</span> <span class="k">with</span>
    <span class="o">|</span> <span class="nc">Lf</span> <span class="o">-&gt;</span> <span class="n">k</span> <span class="p">(</span><span class="mi">0</span> <span class="o">=</span> <span class="n">target</span><span class="p">)</span>
    <span class="o">|</span> <span class="nc">Br</span> <span class="p">(</span><span class="n">v</span><span class="o">,</span> <span class="n">l</span><span class="o">,</span> <span class="n">r</span><span class="p">)</span> <span class="o">-&gt;</span>
        <span class="n">aux</span> <span class="n">l</span> <span class="p">(</span><span class="n">target</span> <span class="o">-</span> <span class="n">v</span><span class="p">)</span> <span class="p">(</span><span class="k">fun</span> <span class="n">lv</span> <span class="o">-&gt;</span>
            <span class="n">aux</span> <span class="n">r</span> <span class="p">(</span><span class="n">target</span> <span class="o">-</span> <span class="n">v</span><span class="p">)</span> <span class="p">(</span><span class="k">fun</span> <span class="n">rv</span> <span class="o">-&gt;</span>
                <span class="k">if</span> <span class="n">lv</span> <span class="k">then</span> <span class="bp">true</span> <span class="k">else</span> <span class="n">k</span> <span class="n">rv</span><span class="p">))</span>
  <span class="k">in</span>
  <span class="n">aux</span> <span class="n">node</span> <span class="n">target</span> <span class="nn">Fun</span><span class="p">.</span><span class="n">id</span></code></pre></figure>

<p>In this new implementation, we insert a conditional within the
continuation to ignore searching down the right subtree.</p>

<h2 id="benchmarks">Benchmarks</h2>
<p>Initializing a balanced binary tree of height=26 =&gt; 67108863 nodes
with a single valid path down the left spine, we get the following
results</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>        recursive: 13.87 WALL <span class="o">(</span>13.87 usr +  0.00 sys <span class="o">=</span> 13.87 CPU<span class="o">)</span> @  0.72/s <span class="o">(</span><span class="nv">n</span><span class="o">=</span>10<span class="o">)</span>
              cps: 18.27 WALL <span class="o">(</span>18.27 usr +  0.00 sys <span class="o">=</span> 18.27 CPU<span class="o">)</span> @  0.55/s <span class="o">(</span><span class="nv">n</span><span class="o">=</span>10<span class="o">)</span>
cps short circuit:  0.00 WALL <span class="o">(</span> 0.00 usr +  0.00 sys <span class="o">=</span>  0.00 CPU<span class="o">)</span> @ 3333333.34/s <span class="o">(</span><span class="nv">n</span><span class="o">=</span>10<span class="o">)</span>
</code></pre></div></div>

<p>Clearly, the test case is skewed to show the best performance of the
<code class="language-plaintext highlighter-rouge">cps short circuiting</code> function. If the valid path was down the right
of the tree, we would get similarly bad performance as the <code class="language-plaintext highlighter-rouge">cps</code>
implementation. What’s interesting is perhaps the bad performance of
our <code class="language-plaintext highlighter-rouge">cps</code> implementation. My guess is that whilst we are saving on
stack memory, we have moved the recursion into heap memory by
allocating pointers onto the heap which is invariably slower.</p>

<p>This finding motivated me to dig into seeing when we actually benefit
from writing stack-saving cps functions. For this particular case, our
recursive call depth is limited by the height of the tree which is
pretty shallow, not nearly big enough to burst the stack. This means
that one needs to consider the actual depth of the recursive calls and
whether they realistically place any stress on the stack. A traversal
over a list is perhaps a more motivating case for a CPS transformation
than our tree here. In fact, I ended up running out of memory just by
the allocation of the tree alone. To which we come to the second
interesting observation, you need to consider that your CPS function
will be competing with your data structure (if any) for resources on
the heap. For most user programs, we don’t usually get that kind of
scale for data structures anyway. It usually makes sense to opt for
the simpler recursive solution.</p>

<p>(In utop and OCaml bytecode programs, the stack limit is 1024k words
whilst natively compiled programs depend on the system limits which
is 8192 in my case)</p>

<blockquote>
  <p>Food for thought: Continuations are an abstraction of the program stack</p>
</blockquote>]]></content><author><name>Koon Wen Lee</name></author><summary type="html"><![CDATA[I’ve always found continuation-passing-style (CPS) one of the more an elusive concept to grasp. Today I came across a simple tree traversal problem that helped me work through some of that complexity.]]></summary></entry></feed>